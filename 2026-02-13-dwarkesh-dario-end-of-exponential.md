# YouTube 视频翻译：Dario Amodei — "We are near the end of the exponential"

## 视频元信息
- **视频ID**: n1E9IZfvGMA
- **视频链接**: https://www.youtube.com/watch?v=n1E9IZfvGMA
- **发布时间**: 2026-02-13
- **频道**: Dwarkesh Patel
- **主题**: AI 扩展定律、AGI 时间线、经济影响、安全与治理

---

## 核心观点
### 1. 指数增长的终局与“大算力团块”假说
Dario 认为我们正接近 AI 能力指数增长的尾声。他多年前提出的“大算力团块（Big Blob of Compute）”假说依然成立：只要有足够的算力、数据量、数据质量和可扩展的目标函数，模型就能不断变强。目前强化学习（RL）也表现出了与预训练类似的缩放规律（Scaling Laws），模型正从特定任务（如数学、代码）向更广泛的能力泛化。

### 2. AGI 时间线：“数据中心里的天才国度”
Dario 预测，我们有 90% 的概率在 10 年内实现“数据中心里的天才国度”（即 AGI），而他的直觉是这可能在 1-3 年内（2026-2028年）发生。这种系统将拥有相当于受过良好教育的人类的通用能力，甚至在某些领域超越诺贝尔奖得主。

### 3. 编码与生产力悖论
AI 编写代码的比例将很快从 90% 达到 100%。虽然生产力提升巨大，但受到“经济扩散（Diffusion）”的限制——即组织惯性、安全审查、流程变更等现实摩擦，导致实际经济价值的释放不会瞬间发生，而是有一个快速但非无限快的过程。

### 4. 行业经济学：利润与寡头垄断
AI 行业目前的推理业务毛利很高，但企业会将大部分利润再投资于下一代模型的训练（成本呈指数级增长）。行业的盈利往往源于对需求的低估；如果高估需求，巨大的算力投入会导致亏损。未来 AI 行业可能形成类似云计算的寡头格局（3-4 家主要玩家），但产品差异化会比云服务更大。

### 5. 持续学习与上下文窗口
对于模型是否需要像人类一样“在岗学习（On-the-job learning）”，Dario 认为这可能不是必需的。超长的上下文窗口（Context Window）加上预训练的泛化能力，可能足以让模型通过阅读大量背景资料瞬间掌握工作所需的上下文，从而替代人类的长期学习过程。

### 6. 地缘政治与出口管制
Dario 坚定支持对中国的芯片出口管制，认为这是国家安全利益所在。他担心 AI 会被威权政权利用来巩固统治，导致“高科技威权主义”难以被推翻。他主张民主国家应在 AI 时代确立“道路规则”，并在谈判中占据主导地位。

### 7. 治理架构与州级监管
面对生物恐怖主义和自主性风险，我们需要建立一种既能保护公民自由又能有效监控风险的“治理架构”。Dario 反对像田纳西州那样禁止“情感 AI”的州级法律，认为这会阻碍 AI 在心理健康等领域的益处，主张由联邦政府制定统一的安全和透明度标准。

### 8. Anthropic 的企业文化
为了避免重蹈其他 AI 实验室内部内斗和分裂的覆辙，Anthropic 极其重视企业文化建设。Dario 通过每两周一次的“Dario Vision Quest”全员演讲和透明的内部沟通，确保 2500 名员工在使命和价值观上保持一致。

---

## 完整中文翻译

### 指数增长的现状与 RL 缩放定律
**Dwarkesh**: 我们三年前聊过。在你看来，过去三年最大的更新是什么？当时的感受和现在最大的区别是什么？

**Dario**: 总体来说，底层技术的指数增长大致如我所料。这里那里可能有一两年的误差。我不知道我是否能预测到代码的具体发展方向。但当我看这个指数曲线时，模型从聪明的高中生到聪明的大学生，再到开始做博士和专业级别的工作，这大致符合我的预期。前沿领域有点参差不齐，但大体上和我预想的一样。
最让我惊讶的是，公众缺乏对我们“有多接近指数增长终点”的认知。对我来说，这绝对是疯狂的：无论是在圈内还是圈外，人们还在谈论那些陈旧的、老生常谈的政治热点问题，而我们却正处于指数增长的尾声。

**Dwarkesh**: 我想了解现在的指数增长是什么样子的。三年前我问你的第一个问题是：“扩展（Scaling）是怎么回事，为什么它有效？”我现在有一个类似的问题，但感觉更复杂了。至少从公众的角度来看，三年前在许多数量级的算力上都有众所周知的公开趋势，你可以看到损失函数是如何下降的。现在我们有了强化学习（RL）的扩展，但没有公开的缩放定律。甚至连它的原理都不清楚。这是在教模型技能吗？是在教元学习吗？现在的缩放假说是什么？

**Dario**: 其实我的假说甚至可以追溯到 2017 年。我想我上次谈到过，我写过一个文档叫“大算力团块假说（The Big Blob of Compute Hypothesis）”。它不仅仅是关于语言模型的扩展。我写的时候 GPT-1 刚出来。那是众多事物中的一个。那时候还有机器人技术。人们试图将推理作为与语言模型分开的东西来研究，还有 OpenAI 的 Dota 和 DeepMind 的 AlphaGo、AlphaStar 中的 RL 扩展。
那个文档写得更通用。Rich Sutton 几年后发表了《苦涩的教训（The Bitter Lesson）》。假说基本上是一样的。它说的是，所有的聪明才智、所有的技巧、所有的“我们需要一种新方法来做某事”，这些都不太重要。只有几件事是重要的。我想我列了七件。
第一是你拥有多少原始算力。第二是数据量。第三是数据的质量和分布，需要广泛的分布。第四是你训练了多久。第五是你需要一个可以扩展到月球的目标函数。预训练目标函数就是这样一个目标函数。另一个是 RL 目标函数，即你有一个目标，你要去达成这个目标。其中有客观奖励（如数学和编码），也有主观奖励（如 RLHF 或更高阶的版本）。第六和第七是关于归一化或调节的东西，只是为了获得数值稳定性，让这个“大算力团块”以层流方式流动，而不是遇到问题。
这就是那个假说，我仍然坚持这个假说。我没看到太多不符合它的东西。预训练缩放定律是我们看到的一个例子，它还在继续。现在大家普遍报道，我们对预训练感觉良好，它继续给我们带来收益。
现在的变化是，我们在 RL 上也看到了同样的事情。我们看到了一个预训练阶段，然后是一个 RL 阶段。对于 RL，其实是一样的。甚至其他公司也在发布中提到：“我们在数学竞赛（如 AIME）上训练模型，模型表现的好坏与我们训练它的时间呈对数线性关系。”我们也看到了这一点，不仅仅是数学竞赛，而是广泛的 RL 任务。我们在 RL 中看到了与预训练相同的缩放规律。

### 进化、人类学习与 AI 的位置
**Dwarkesh**: 你提到了 Rich Sutton 和《苦涩的教训》。我去年采访了他，他其实非常不看好 LLM。我不知道这是否是他的观点，但一种反驳意见是：如果一个东西拥有人类学习的真正核心，它就不需要这数十亿美元的数据和算力以及这些定制的环境来学习如何使用 Excel、PowerPoint 或浏览器。我们需要通过这些 RL 环境构建这些技能，这暗示我们实际上缺乏核心的人类学习算法。所以我们扩展错了东西。这确实提出了一个问题：如果我们认为有一种像人类一样能即时学习的东西，为什么我们要进行所有这些 RL 扩展？

**Dario**: 我认为这把几个应该分开思考的事情混在一起了。这里确实有一个谜题，但它可能并不重要。事实上，我猜它可能真的不重要。让我先把 RL 拿开一秒钟，因为我实际上认为说 RL 在这方面与预训练有什么不同是一个转移注意力的说法。
如果我们看预训练的扩展，2017 年 Alec Radford 做 GPT-1 时非常有趣。GPT-1 之前的模型是在不代表广泛文本分布的数据集上训练的。你有非常标准的语言建模基准。GPT-1 本身是在一堆同人小说（Fanfiction）上训练的。它是文学文本，只是你能获得的文本的一小部分。那时候大概是十亿个词之类的，所以小数据集代表了你在世界上能看到的相当狭窄的分布。它的泛化能力不好。如果你在某个同人小说语料库上做得更好，它并不能很好地泛化到其他任务。
只有当你在这个互联网上的所有任务上进行训练——当你从 Common Crawl 或 Reddit 链接（我们在 GPT-2 中做的）进行通用互联网抓取时——你才开始获得泛化能力。
我认为我们在 RL 上看到了同样的事情。我们首先从简单的 RL 任务开始，比如在数学竞赛上训练，然后转移到涉及代码等更广泛的训练。现在我们正在转移到许多其他任务。我认为我们将越来越多地获得泛化能力。所以这在某种程度上消除了 RL 与预训练的对立。
但这两种情况都有一个谜题，那就是在预训练中我们使用了数万亿个 Token。人类看不到数万亿个词。所以这里确实存在样本效率的差异。确实有些不同。模型从零开始，它们需要更多的训练。
但我们也看到，一旦它们训练好了，如果我们给它们一百万的上下文长度——唯一阻碍长上下文的是推理成本——它们非常擅长在该上下文中学习和适应。
所以我不知道完整的答案。我认为发生的事情是，预训练不像人类学习的过程，它介于人类学习的过程和人类进化的过程之间。我们从进化中获得许多先验知识。我们的大脑不是一张白纸。关于这一点已经写了很多书。语言模型更像是一张白纸。它们实际上是从随机权重开始的，而人类大脑开始时所有这些区域都连接到所有这些输入和输出。
也许我们应该把预训练——就此而言，RL 也是——看作是存在于人类进化和人类现场学习之间的中间空间的东西。我们应该把模型所做的上下文学习（In-context learning）看作是介于人类长期学习和短期学习之间的东西。
所以有一个层级：有进化，有长期学习，有短期学习，还有人类的反应。LLM 的阶段存在于这个光谱上，但不一定在完全相同的点上。LLM 落在这些点之间，没有对应于人类某些学习模式的类比。这说得通吗？

**Dwarkesh**: 是的，虽然有些事情还是有点令人困惑。例如，如果类比是这就像进化，所以样本效率低也没关系，那么如果我们要从上下文学习中获得超级样本高效的智能体（Agent），为什么我们要费心构建所有这些 RL 环境？有些公司的工作似乎是教模型如何使用这个 API，如何使用 Slack 等等。如果那种能即时学习的智能体正在出现或已经出现，我不明白为什么要如此强调这些。

**Dario**: 我不能代表其他人的重点。我只能谈谈我们是如何思考的。目标不是在 RL 中教模型每一个可能的技能，就像我们在预训练中不那样做一样。在预训练中，我们并不试图让模型接触到单词组合的每一种可能方式。相反，模型在很多东西上训练，然后在预训练中实现泛化。
这就是我亲眼看到的从 GPT-1 到 GPT-2 的转变。我有过这样的时刻：“哦，是的，你只要给模型一列数字——这是房子的成本，这是房子的平方英尺——模型就会完成模式并进行线性回归。”虽然不是很好，但它做到了，而且它以前从未见过完全相同的东西。
所以，就我们构建这些 RL 环境而言，目标与五到十年前预训练所做的非常相似。我们试图获取大量数据，不是因为我们想覆盖特定的文档或特定的技能，而是因为我们想泛化。

### AGI 时间线与“天才国度”
**Dwarkesh**: 你提出的框架显然是有道理的。我们正在向 AGI 迈进。此时没有人不同意我们将在本世纪实现 AGI。关键是你认为我们正在接近指数增长的终点。其他人看这个问题会说：“我们从 2012 年开始取得进展，到 2035 年我们将拥有类人智能体。”显然，我们在这些模型中看到了进化所做的事情，或者人类一生中学习所做的事情。我想了解你看到了什么，让你认为这还有一年而不是十年？

**Dario**: 这里你可以提出两个主张，一个较强，一个较弱。
从较弱的主张开始，当我在 2019 年第一次看到扩展时，我不确定。这是一个 50/50 的事情。我以为我看到了一些东西。我的主张是，这比任何人想象的都要可能得多。也许有 50% 的机会发生。
基于基本假设，正如你所说，在十年内我们将达到我所说的“数据中心里的天才国度（Country of Geniuses in a Data Center）”，我有 90% 的把握。很难比 90% 更高，因为世界是如此不可预测。也许不可减少的不确定性让我们处于 95%，你会遇到像多家公司内部动荡、台湾被入侵、所有晶圆厂被导弹炸毁这样的事情。

**Dwarkesh**: 现在你乌鸦嘴了，Dario。

**Dario**: 你可以构建一个 5% 的世界，事情被推迟十年。还有另外 5% 是我对可验证任务非常有信心。对于编码，除了那种不可减少的不确定性，我认为我们在一两年内就会达到那个水平。在能够进行端到端编码方面，我们不可能在十年内达不到。
我唯一的一点根本不确定性，即使在很长的时间尺度上，是关于那些不可验证的任务：规划火星任务；做一些基础科学发现，如 CRISPR；写小说。这些任务很难验证。我几乎可以肯定我们有一条可靠的路径可以到达那里，但如果有一点不确定性，那就是在那里。
在十年的时间线上，我有 90% 的把握，这几乎是你所能达到的最高确定性。我认为说这不会在 2035 年之前发生是疯狂的。在某个理智的世界里，这将是非主流观点。

**Dwarkesh**: 但对验证的强调暗示了我对这些模型是否泛化的缺乏信念。如果你想想人类，我们在获得可验证奖励的事情和没有获得奖励的事情上都很擅长。

**Dario**: 不，这就是为什么我几乎确定的原因。我们已经看到从可验证事物到不可验证事物的实质性泛化。我们已经看到了。

**Dwarkesh**: 但你似乎把这强调为一个光谱，在这个光谱上我们将区分我们在哪些领域看到更多进展。这似乎不是人类变得更好的方式。

**Dario**: 我们达不到那个目标的世界，是我们做了所有可验证的事情，其中许多都泛化了，但我们没有完全达到那个目标。我们没有完全填满盒子的另一边。这不是二元对立的。即使泛化很弱，你只能做可验证的领域，我也不清楚在这样的世界里你是否能自动化软件工程。你在某种意义上是“软件工程师”，但作为软件工程师的一部分工作包括写关于你宏伟愿景的长备忘录。我不认为那是 SWE 工作的一部分。那是公司工作的一部分，而不是 SWE 特有的。但 SWE 确实涉及设计文档和其他类似的东西。模型在写注释方面已经相当不错了。
再次强调，我在这里做出的主张比我相信的要弱得多，以区分两件事。我们在软件工程方面已经快到了。

### 生产力、扩散与经济影响
**Dwarkesh**: 用什么指标？有一个指标是 AI 写了多少行代码。如果你考虑软件工程历史上的其他生产力提高，编译器编写了所有的软件行。AI 写了多少行与生产力提高了多少是有区别的。“我们快到了”意味着……生产力提高有多大，而不仅仅是 AI 写了多少行？

**Dario**: 我其实同意你的观点。我对代码和软件工程做了一系列预测。我认为人们一再误解了它们。让我列出这个光谱。
大约八九个月前，我说 AI 模型将在三到六个月内编写 90% 的代码行。这发生了，至少在某些地方。它在 Anthropic 发生了，在许多使用我们模型的下游人员那里也发生了。但这实际上是一个非常弱的标准。人们以为我是说我们将不需要 90% 的软件工程师。这两者是天壤之别。
光谱是：90% 的代码由模型编写，100% 的代码由模型编写。这在生产力上有很大差异。90% 的端到端 SWE 任务——包括编译、设置集群和环境、测试功能、编写备忘录——由模型完成。100% 的今天的 SWE 任务由模型完成。
即使发生这种情况，也不意味着软件工程师失业了。他们可以做新的更高层次的事情，他们可以进行管理。然后在这个光谱的更远处，是对 SWE 的需求减少 90%，我认为这会发生，但这是一个光谱。我在《技术的青春期（The Adolescence of Technology）》中写到了这一点，我用农业经历了这种光谱。
我其实完全同意你的观点。这些基准彼此非常不同，但我们正在超快地通过它们。

**Dwarkesh**: 你的部分愿景是，从 90 到 100 将发生得很快，并且会导致巨大的生产力提高。但我注意到，即使在绿地项目（Greenfield projects）中，人们开始使用 Claude Code 或其他东西，人们报告开始了很多项目……我们在外面的世界看到软件复兴了吗？所有这些本来不存在的新功能？至少到目前为止，似乎我们没有看到。这确实让我怀疑。即使我从未不得不干预 Claude Code，世界是复杂的。工作是复杂的。闭环自包含系统，无论是仅仅编写软件还是其他什么，仅仅从中我们会看到多广泛的收益？也许这应该稀释我们对“天才国度”的估计。

**Dario**: 我同时同意你的观点，这是这些事情不会瞬间发生的原因，但同时，我认为效果会非常快。你可以有两个极端。一个是 AI 不会取得进展。它很慢。它在经济中扩散需要永远的时间。“经济扩散（Economic diffusion）”已经成为这些流行词之一，用来解释为什么我们不会取得 AI 进展，或者为什么 AI 进展并不重要。
另一个轴是我们会得到递归的自我改进，整个事情。你不能就在曲线上画一条指数线吗？我们在递归后多少纳秒内就会在太阳周围拥有戴森球。我完全是在讽刺这种观点，但有这两个极端。
但我们从一开始就看到的，至少如果你看 Anthropic 内部，我们看到了这种离奇的每年 10 倍的收入增长。所以在 2023 年，它是从零到 1 亿美元。在 2024 年，它是从 1 亿美元到 10 亿美元。在 2025 年，它是从 10 亿美元到 90-100 亿美元。

**Dwarkesh**: 你们应该直接买 10 亿美元自己的产品，这样你们就可以……

**Dario**: 而今年的第一个月，那个指数是……你会认为它会放缓，但我们在 1 月份又增加了几十亿的收入。显然那条曲线不能永远持续下去。GDP 只有这么大。我甚至猜今年它会稍微弯曲，但那是一条快速曲线。那是一条非常快的曲线。我敢打赌，即使规模扩大到整个经济，它也会保持相当快。
所以我认为我们应该思考这个中间世界，事情非常快，但不是瞬间的，因为经济扩散，因为需要闭环，它们需要时间。因为它很繁琐：“我必须在我的企业内进行变革管理……我设置了这个，但我必须更改安全权限才能使其真正工作……我有这个旧软件，在模型编译和发布之前检查它，我必须重写它。是的，模型可以做到这一点，但我必须告诉模型去做。这需要时间。”
所以我认为我们目前看到的一切都与这样一种观点相容：有一个快速的指数是模型的能力。然后有另一个快速的指数是它的下游，即模型向经济的扩散。不是瞬间的，不是缓慢的，比以前的任何技术都快得多，但它有其局限性。当我看 Anthropic 内部，当我看我们的客户时：采用很快，但不是无限快。

### 行业经济学：利润、亏损与预测
**Dwarkesh**: 你告诉投资者，你们计划从 2028 年开始盈利。这一年我们可能会得到作为数据中心的天才国度。这将解锁医学、健康和新技术的进步。这难道不是你应该重新投资业务并建立更大的“国家”以便他们能做出更多发现的时候吗？

**Dario**: 在这个领域，盈利能力是一件奇怪的事情。我不认为在这个领域，盈利能力实际上是衡量支出与投资业务的标准。让我们拿一个模型来说。我实际上认为，当你低估了你将获得的需求量时，就会发生盈利；当你高估了你将获得的需求量时，就会发生亏损，因为你是提前购买数据中心的。
这样想吧。同样，这些是程式化的事实。这些数字并不准确。我只是想做一个玩具模型。假设你的一半算力用于训练，一半算力用于推理。推理有一些超过 50% 的毛利率。所以这意味着如果你处于稳态，你建立一个数据中心，如果你确切知道你得到的需求，你会得到一定数量的收入。
假设你每年支付 1000 亿美元用于算力。在 500 亿美元上，你支持 1500 亿美元的收入。另外 500 亿美元用于训练。基本上你是盈利的，你赚了 500 亿美元的利润。这就是今天行业的经济学，或者不是今天，而是我们预测的一两年后的情况。
唯一让这不成立的是如果你得到的需求少于 500 亿美元。那么你有超过 50% 的数据中心用于研究，你就没有盈利。所以你训练了更强的模型，但你没有盈利。如果你得到的需求比你想象的多，那么研究就会受到挤压，但你能够支持更多的推理，你就更盈利了。
我想说的是，你先决定算力的数量。然后你有一些推理与训练的目标愿望，但这由需求决定。它不由你决定。

**Dwarkesh**: 我听到的是，你预测利润的原因是你系统性地投资不足？

**Dario**: 不，不，不。我是说这很难预测。关于 2028 年以及何时会发生，那是我们尽力向投资者做出的尝试。所有这些东西都因为不确定性锥体而非常不确定。如果收入增长足够快，我们可能会在 2026 年盈利。如果我们高估或低估了明年，那可能会剧烈波动。

**Dwarkesh**: 我想说的是，你脑子里有一个商业模型，投资，投资，投资，获得规模，然后变得盈利。有一个点事情会好转。我不认为这个行业的经济学是那样运作的。

**Dario**: 我明白了。所以如果我理解正确的话，你是说因为我们应该获得的算力数量和我们实际获得的算力数量之间的差异，我们在某种程度上被迫盈利。但这并不意味着我们将继续盈利。我们将把钱再投资，因为现在 AI 取得了如此大的进步，我们想要一个更大的天才国度。所以回到收入很高，但损失也很高。
如果每年我们都准确预测需求是多少，我们将每年都盈利。因为花费 50% 的算力用于研究，加上高于 50% 的毛利率和正确的需求预测会导致利润。这就是我认为存在的盈利商业模式，但被这些提前建设和预测错误所掩盖。

### 地缘政治、中国与威权主义
**Dwarkesh**: 你提到了出口管制。为什么美国和中国不应该都拥有“数据中心里的天才国度”？为什么这不会发生或者为什么这不应该发生？

**Dario**: 为什么不应该发生。如果这真的发生了，我们可能会遇到几种情况。如果我们处于进攻主导（Offense-dominant）的局势，我们可能会遇到像核武器那样的情况，但更危险。任何一方都可以轻易摧毁一切。
我们也可能有一个不稳定的世界。核平衡是稳定的，因为它是威慑。但假设对于如果两个 AI 打架谁会赢存在不确定性？这可能会造成不稳定。当双方对自己获胜的可能性有不同的评估时，你经常会发生冲突。如果一方认为“哦，是的，我有 90% 的机会赢”，而另一方也这么认为，那么打架的可能性就大得多。他们不可能都对，但他们都可以这么想。

**Dwarkesh**: 但这似乎是一个完全通用的反对 AI 技术扩散的论点。

**Dario**: 让我继续说，因为我认为我们最终会得到扩散。我的另一个担忧是政府会用 AI 压迫自己的人民。我担心这样一个世界：你有一个国家，那里已经有一个正在建立高科技威权国家的政府。
明确一点，这是关于政府的。这不是关于人民的。我们需要找到一种方法让各地的人们受益。我的担忧是关于政府的。我的担忧是，如果世界被分成两块，其中一块可能是威权或极权主义的，以一种很难被取代的方式。
现在，政府最终会获得强大的 AI 吗？是否存在威权主义的风险？是的。政府最终会获得强大的 AI 吗？是否存在糟糕均衡的风险？是的，我认为这两件事都有。但初始条件很重要。在某个时候，我们需要制定道路规则。
我不是说一个国家，无论是美国还是民主联盟——我认为这将是一个更好的设置，尽管这需要比我们目前似乎想要进行的更多的国际合作——应该只是说，“这些是道路规则。”会有一些谈判。世界将不得不应对这个问题。
我希望的是，世界上的民主国家——那些政府更接近亲人类价值观的国家——在制定道路规则时持有更强的手牌并拥有更多的筹码。所以我非常关心那个初始条件。

### 治理、宪法与安全
**Dwarkesh**: 你们最近宣布 Claude 将拥有一部符合一套价值观的宪法（Constitution），而不一定只是符合最终用户的意愿。我可以想象一个世界，如果它与最终用户对齐，它会保留我们今天世界的力量平衡，因为每个人都有自己的 AI 为他们辩护。坏人与好人的比例保持不变。这似乎对我们今天的世界有效。为什么不这样做，而是要有一套 AI 应该遵循的特定价值观更好？

**Dario**: 我不确定我会那样划分区别。这里可能有两个相关的区别。我认为你在谈论两者的混合。一个是，我们应该给模型一套关于“做这个”与“不做这个”的指令吗？另一个是，我们应该给模型一套行动原则吗？
这纯粹是我们观察到的实践和经验。通过教模型原则，让它从原则中学习，它的行为更一致，更容易覆盖边缘情况，模型更有可能做人们想让它做的事情。换句话说，如果你给它一列规则——“不要告诉人们如何偷车，不要说韩语”——它并不真正理解规则，而且很难从规则中泛化。这只是一列该做和不该做的事。
而如果你给它原则——它有一些硬性护栏，比如“不要制造生物武器”，但——总的来说，你试图理解它应该以什么为目标，它应该如何运作。所以仅从实践角度来看，这被证明是一种更有效的训练模型的方法。这就是规则与原则的权衡。
然后是你谈到的另一件事，即修正性（Corrigibility）与内在动机的权衡。模型应该在多大程度上是一个“皮囊（Skin suit）”，只是直接遵循给它指令的人的指令，还是模型应该有一套内在的价值观并自己去做事情？
在这方面，我实际上会说关于模型的一切都更接近于它应该主要做人们想要它做的事情。它应该主要遵循指令。我们并不是试图建立某种自己去管理世界的东西。我们实际上非常偏向修正性一边。
现在，我们确实说有些事情模型不会做。我想我们在宪法中以各种方式说过，在正常情况下，如果有人要求模型做一个任务，它应该做那个任务。那应该是默认的。但如果你要求它做危险的事情，或者伤害别人，那么模型就不愿意那样做。所以我实际上把它看作是一个主要可修正的模型，有一些限制，但这些限制是基于原则的。

### Anthropic 的文化与“Dario Vision Quest”
**Dwarkesh**: 最后一个问题。通常科技公司的 CEO 不会每隔几个月写 50 页的备忘录。似乎你已经设法为自己建立了一个角色，并在你周围建立了一个与这种更具智识类型的 CEO 角色相容的公司。我想了解你是如何构建这一点的。那是怎么运作的？你只是离开几周，然后告诉你的公司，“这是备忘录。这是我们要做的”？据报道，你在内部也写了很多这样的东西。

**Dario**: 对于这一篇，我是寒假期间写的。我很难找到时间真正写它。但我从更广泛的角度思考这个问题。我认为这与公司的文化有关。我大概花三分之一，也许 40% 的时间确保 Anthropic 的文化是好的。
随着 Anthropic 变得越来越大，直接参与模型的训练、模型的发布、产品的构建变得越来越难。有 2500 人。我有某些直觉，但很难参与每一个细节。我尽可能尝试，但有一件事是非常有杠杆作用的，那就是确保 Anthropic 是一个工作的好地方，人们喜欢在那里工作，每个人都认为自己是团队成员，每个人都合作而不是相互对抗。
我们看到随着其他一些 AI 公司的成长——不点名——我们开始看到退相干（Decoherence）和人们相互争斗。我会说从一开始就有很多这样的事情，但现在变得更糟了。我认为我们做得非常好，即使不是完美，也能把公司凝聚在一起，让每个人都感受到使命，我们对使命是真诚的，每个人都相信那里的其他人都是为了正确的理由而工作。我们是一个团队，人们不是试图以牺牲他人为代价来获得晋升或在背后捅刀子，这在其他一些地方经常发生。
你是怎么做到的？很多事情。是我，是负责公司日常运营的 Daniela，是联合创始人，是我们雇佣的其他人，是我们试图创造的环境。但我认为文化中很重要的一点是，其他领导者也是，尤其是必须阐明公司是关于什么的，为什么它在做它正在做的事情，它的战略是什么，它的价值观是什么，它的使命是什么，它代表什么。
当你达到 2500 人时，你不能逐个人去做。你必须写，或者你必须对整个公司说。这就是为什么我每两周在全公司面前讲一个小时。我不会说我在内部写文章。我做两件事。第一，我写这个叫 DVQ 的东西，Dario Vision Quest。
不是我起的名字。这是它收到的名字，这是我试图反对的名字之一，因为它听起来像是我去吸食佩奥特掌（致幻剂）之类的。但这个名字就这样叫开了。所以我每两周在公司面前讲一次。我有一份三四页的文件，我就讲三四个不同的话题，关于内部发生了什么，我们正在生产的模型，产品，外部行业，整个世界与 AI 的关系以及一般的地缘政治。只是这些的混合。我非常诚实地讲，我说，“这就是我在想的，这就是 Anthropic 领导层在想的，”然后我回答问题。
这种直接联系有很多价值，当你把事情沿着链条向下传递六层时很难实现。很大一部分公司员工会参加，无论是现场还是虚拟的。这真的意味着你可以交流很多。
我做的另一件事是，我在 Slack 上有一个频道，我只是写一堆东西并经常评论。通常这是为了回应我在公司看到的事情或人们问的问题。我们做内部调查，人们有些担心的事情，我会把它们写下来。我对这些事情非常诚实。我只是非常直接地说出来。
重点是建立一个告诉公司关于正在发生的事情的真相的声誉，实事求是，承认问题，避免那种“公司腔（Corpo speak）”，那种在公共场合通常必要的防御性沟通，因为世界很大，充满了恶意解读的人。但如果你有一家你信任的人的公司，我们也试图雇佣我们信任的人，那么你真的可以完全不过滤。
我认为这是公司的一个巨大优势。它使它成为一个更好的工作场所，使人们不仅仅是各部分的总和，并增加了我们完成使命的可能性，因为每个人都在使命上达成一致，每个人都在辩论和讨论如何最好地完成使命。

**Dwarkesh**: 好了，代替外部的 Dario Vision Quest，我们有了这次采访。这有点像那个。这很有趣，Dario。谢谢你这么做。

**Dario**: 谢谢你，Dwarkesh。

---
**翻译说明**：
- 翻译基于视频英文字幕逐字稿
- 部分术语采用保留英文或音译的方式
- 演讲中的幽默和口语化表达已尽量转化为自然的中文表达
